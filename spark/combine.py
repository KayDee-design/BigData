from pyspark.sql import SparkSession

# === ðŸ”¹ Start Spark Session ===
spark = SparkSession.builder.appName("Combine Posts and Comments").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")

# === ðŸ”¹ Load CSVs ===
comments_df = spark.read.csv("hdfs://sentiment-namenode-1:8020/input/comments.csv", header=True, inferSchema=True)
posts_df = spark.read.csv("hdfs://sentiment-namenode-1:8020/input/posts.csv", header=True, inferSchema=True)

# === ðŸ”¹ Rename Columns for Clarity ===
comments_df = comments_df.withColumnRenamed("Comment", "Comments")
posts_df = posts_df.withColumnRenamed("Content", "Posts")

# === ðŸ”¹ Join on Post_ID ===
combined_df = comments_df.join(posts_df, on="Post_ID", how="inner")

# === ðŸ”¹ Select Only Required Columns ===
final_df = combined_df.select("Post_ID", "Posts", "Date", "Comments", "Sentiment")

# === ðŸ”¹ Show Preview with Truncation
print("\n Combined Preview (Truncated):")
final_df.show(5)

# === ðŸ”¹ Save Combined Raw Data
# final_df.coalesce(1).write.csv("/opt/spark/combineddata_raw.csv", header=True, mode="overwrite")

print("\n Combined raw data saved to: /opt/spark/combineddata_raw.csv")
